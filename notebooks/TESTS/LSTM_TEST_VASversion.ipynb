{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "from src.common.analysis_and_plots import Visualize as V\n",
    "from src.features.build_features import FeatureEngineering as FE\n",
    "from src.common.globals import G\n",
    "from src.common.globals import split_train_valid_test, get_naive_forecast, calc_errors, save_errors_to_table\n",
    "from src.data.get_data import CSVsLoader\n",
    "from src.common.logs import setup_logging, log_model_info\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "logger = setup_logging(logger_name=__name__,\n",
    "                        console_level=logging.INFO, \n",
    "                        log_file_level=logging.INFO)\n",
    "\n",
    "PROJECT_PATH = G.get_project_root()\n",
    "DATA_DIR_PROCESSED = os.path.join(PROJECT_PATH, r'data\\03_processed\\daily_full')\n",
    "\n",
    "config = {\n",
    "    'AV': {\n",
    "        'key': '',\n",
    "        'ticker': 'MSFT',\n",
    "        'outputsize': 'full',\n",
    "        'key_adjusted_close': 'Adj Close',\n",
    "        'key_volume': 'Volume',\n",
    "    },\n",
    "    'data': {\n",
    "        'test_size': 0.05,\n",
    "    }, \n",
    "    'model': {\n",
    "        'name': 'LSTM', \n",
    "        'window': 20,\n",
    "        'batch_size' : 32,\n",
    "        'shuffle_buffer_size' : 5600, # https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\n",
    "        'epochs' : 100,\n",
    "        'optimizer': tf.keras.optimizers.Adam(),\n",
    "        'loss': tf.keras.losses.Huber(),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_column_to_end(df, last_column):\n",
    "    ''' \n",
    "    Changes positions of columns in df to put the target column at the end\n",
    "\n",
    "    Args:\n",
    "        df (pandas dataframe) - dataframe to change\n",
    "        last_column (string) - name of the column to put at the end\n",
    "        \n",
    "    Returns:\n",
    "        df (pandas dataframe) - dataframe with the target column at the end\n",
    "    '''\n",
    "    cols = df.columns.tolist()\n",
    "    cols.remove(last_column)\n",
    "    cols.append(last_column)\n",
    "    return df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 15:39:24 - src.data.get_data - INFO - Loaded \"..\\P7-Trading-Bot\\data\\03_processed\\daily_full\\MSFT-daily-full.csv\". Number data points 5995. From \"1999-11-01 00:00:00\" to \"2023-08-29 00:00:00\"\n"
     ]
    }
   ],
   "source": [
    "def windowed_dataset_X(df, window_size, batch_size, shuffle_buffer, verbose=True):\n",
    "    # change the position of the target column to the end\n",
    "    df = label_column_to_end(df, 'Adj Close')\n",
    "\n",
    "    X_df = df.iloc[:, :-1]\n",
    "    y_df = df.iloc[:, -1:]\n",
    "    for col in X_df.columns:\n",
    "        scaler = MinMaxScaler()\n",
    "        X_df[col] = scaler.fit_transform(X_df[col].values.reshape(-1,1))\n",
    "    \n",
    "    # Creating X and y\n",
    "    X = X_df.values\n",
    "    y = y_df.values\n",
    "    if verbose:\n",
    "        print('---------------------------------X,y shape-------------------------------------')\n",
    "        print (f'X.shape: {X.shape}, y.shape: {y.shape}')\n",
    "        print('-'*100)\n",
    "\n",
    "\n",
    "    # Generate a TF Dataset from the series values\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "    if verbose:\n",
    "        print('--------------------------from_tensor_slices--------------------------')\n",
    "        for element in dataset:\n",
    "            print(element)\n",
    "            break\n",
    "        print('-'*100)\n",
    "\n",
    "    # Window the data but only take those with the specified size\n",
    "    # And add + 1 to the window size to account for the label, which we will separate later\n",
    "    dataset = dataset.window(window_size, shift=1, drop_remainder=True)\n",
    "    if verbose:\n",
    "        print('-------------------------------window-----------------------------------')\n",
    "        for window in dataset:\n",
    "            print(type(window))\n",
    "            print(list(window.as_numpy_iterator()))\n",
    "            break\n",
    "        print('-'*100)\n",
    "    \n",
    "    # Flatten the windows by putting its elements in a single batch\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size))\n",
    "    if verbose:\n",
    "        print('--------------------------------flat_map--------------------------------')\n",
    "        for window in dataset:\n",
    "            print(window)\n",
    "            break\n",
    "        print('-'*100)\n",
    "\n",
    "    # # Shuffle the windows\n",
    "    # dataset = dataset.shuffle(shuffle_buffer)\n",
    "\n",
    "    # Create batches of windows\n",
    "    # dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    # if verbose:\n",
    "    #     print('--------------------------------batch-----------------------------------')\n",
    "    #     for x in dataset:\n",
    "    #         print(x.numpy().shape)\n",
    "    #         print(x.numpy())\n",
    "    #         break\n",
    "    #     print('-'*100)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# -----------------------------Data----------------------------------------\n",
    "df = CSVsLoader(ticker=config['AV']['ticker'], directory=DATA_DIR_PROCESSED)\n",
    "\n",
    "test_size_int = int(len(df) * config['data']['test_size'])\n",
    "df_train = df.iloc[:-test_size_int].copy(deep=True)\n",
    "df_test = df.iloc[-test_size_int:].copy(deep=True)\n",
    "\n",
    "for col in df.columns:\n",
    "    df_train[f'{col} - 1'] = df_train[col].shift(1)\n",
    "    df_test[f'{col} - 1'] = df_test[col].shift(1)\n",
    "df_train = df_train.dropna()\n",
    "\n",
    "#Drop Original columns is any left \n",
    "df_train = df_train.drop(columns=['Volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset_y(df, window_size, batch_size, shuffle_buffer, verbose=True):\n",
    "    # change the position of the target column to the end\n",
    "    df = label_column_to_end(df, 'Adj Close')\n",
    "\n",
    "    # Creating X and y\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1:].values\n",
    "    if verbose:\n",
    "        print('---------------------------------X,y shape-------------------------------------')\n",
    "        print (f'X.shape: {X.shape}, y.shape: {y.shape}')\n",
    "        print('-'*100)\n",
    "    \n",
    "    # Generate a TF Dataset from the series values\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(y)\n",
    "    if verbose:\n",
    "        print('--------------------------from_tensor_slices--------------------------')\n",
    "        for element in dataset:\n",
    "            print(element)\n",
    "            break\n",
    "        print('-'*100)\n",
    "\n",
    "    # calculate number of points we need to cut to make series evenly divisible by window_size\n",
    "    remainder = window_size - 1\n",
    "\n",
    "    # Remove the reminder elements from the end of dataset\n",
    "    dataset = dataset.take(len(y) - remainder)\n",
    "    if verbose:\n",
    "        print('--------------------------------take len(y)-reminder--------------------------------')\n",
    "        for window in dataset:\n",
    "            print(window)\n",
    "            break\n",
    "        print('-'*100)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------X,y shape-------------------------------------\n",
      "X.shape: (5695, 2), y.shape: (5695, 1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--------------------------from_tensor_slices--------------------------\n",
      "tf.Tensor([0.05339881 0.0355088 ], shape=(2,), dtype=float64)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-------------------------------window-----------------------------------\n",
      "<class 'tensorflow.python.data.ops.dataset_ops._VariantDataset'>\n",
      "[array([0.05339881, 0.0355088 ]), array([0.05358007, 0.02960297]), array([0.05304583, 0.0280377 ]), array([0.05280732, 0.03634458]), array([0.05262606, 0.04995357]), array([0.05108055, 0.19832283]), array([0.05005975, 0.08379013]), array([0.04839022, 0.04928543]), array([0.05077526, 0.04919076]), array([0.05036504, 0.0322219 ]), array([0.04827574, 0.03022788]), array([0.04857148, 0.04055322]), array([0.04636771, 0.04709268]), array([0.04631046, 0.0451055 ]), array([0.04732172, 0.03975076]), array([0.05095653, 0.06740842]), array([0.05077526, 0.0504833 ]), array([0.05275008, 0.03594438]), array([0.05220629, 0.0143648 ]), array([0.05131905, 0.03397001])]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--------------------------------flat_map--------------------------------\n",
      "tf.Tensor(\n",
      "[[0.05339881 0.0355088 ]\n",
      " [0.05358007 0.02960297]\n",
      " [0.05304583 0.0280377 ]\n",
      " [0.05280732 0.03634458]\n",
      " [0.05262606 0.04995357]\n",
      " [0.05108055 0.19832283]\n",
      " [0.05005975 0.08379013]\n",
      " [0.04839022 0.04928543]\n",
      " [0.05077526 0.04919076]\n",
      " [0.05036504 0.0322219 ]\n",
      " [0.04827574 0.03022788]\n",
      " [0.04857148 0.04055322]\n",
      " [0.04636771 0.04709268]\n",
      " [0.04631046 0.0451055 ]\n",
      " [0.04732172 0.03975076]\n",
      " [0.05095653 0.06740842]\n",
      " [0.05077526 0.0504833 ]\n",
      " [0.05275008 0.03594438]\n",
      " [0.05220629 0.0143648 ]\n",
      " [0.05131905 0.03397001]], shape=(20, 2), dtype=float64)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lenght of X = 5676\n"
     ]
    }
   ],
   "source": [
    "train_dataset_X = windowed_dataset_X(df_train, \n",
    "                                    window_size=config['model']['window'], \n",
    "                                    batch_size=config['model']['batch_size'], \n",
    "                                    shuffle_buffer=config['model']['shuffle_buffer_size'],\n",
    "                                    verbose=True)\n",
    "print('Lenght of X =', len(list(train_dataset_X.as_numpy_iterator())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------X,y shape-------------------------------------\n",
      "X.shape: (5695, 2), y.shape: (5695, 1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--------------------------from_tensor_slices--------------------------\n",
      "tf.Tensor([28.80933532], shape=(1,), dtype=float64)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--------------------------------take len(y)-reminder--------------------------------\n",
      "tf.Tensor([28.80933532], shape=(1,), dtype=float64)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lenght of y = 5676\n"
     ]
    }
   ],
   "source": [
    "train_dataset_y = windowed_dataset_y(df_train, \n",
    "                                     window_size=config['model']['window'], \n",
    "                                     batch_size=config['model']['batch_size'], \n",
    "                                     shuffle_buffer=config['model']['shuffle_buffer_size'], \n",
    "                                     verbose=True)\n",
    "print('Lenght of y =', len(list(train_dataset_y.as_numpy_iterator())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (32, 20, 2), y.shape: (32, 1)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.zip((train_dataset_X, train_dataset_y))\n",
    "train_dataset = train_dataset.shuffle(config['model']['shuffle_buffer_size'])\n",
    "train_dataset = train_dataset.batch(config['model']['batch_size']).prefetch(1)\n",
    "\n",
    "for x, y in train_dataset:\n",
    "    print (f'x.shape: {x.numpy().shape}, y.shape: {y.numpy().shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (None, None, 2)\n"
     ]
    }
   ],
   "source": [
    "input_shape = train_dataset.element_spec[0].shape\n",
    "print(f'Input shape: {input_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 15:39:27 - __main__ - INFO - ==============================================================================================\n",
      "2023-09-20 15:39:27 - __main__ - INFO - =================================== MODEL CONFIG AND SETUP ===================================\n",
      "2023-09-20 15:39:27 - __main__ - INFO - ==============================================================================================\n",
      "2023-09-20 15:39:27 - __main__ - INFO - AV_key: , AV_ticker: MSFT, AV_outputsize: full, AV_key_adjusted_close: Adj Close, AV_key_volume: Volume, \n",
      "2023-09-20 15:39:27 - __main__ - INFO - data_test_size: 0.05, \n",
      "2023-09-20 15:39:27 - __main__ - INFO - ----------------------------------------------------------------------\n",
      "2023-09-20 15:39:27 - __main__ - INFO - model_name: LSTM\n",
      "2023-09-20 15:39:27 - __main__ - INFO - model_window: 20\n",
      "2023-09-20 15:39:27 - __main__ - INFO - model_batch_size: 32\n",
      "2023-09-20 15:39:27 - __main__ - INFO - model_shuffle_buffer_size: 5600\n",
      "2023-09-20 15:39:27 - __main__ - INFO - model_epochs: 100\n",
      "2023-09-20 15:39:27 - __main__ - INFO - model_optimizer: <keras.src.optimizers.adam.Adam object at 0x000001C4F1299A50>\n",
      "2023-09-20 15:39:27 - __main__ - INFO - model_loss: <keras.src.losses.Huber object at 0x000001C4F12B38D0>\n",
      "2023-09-20 15:39:27 - __main__ - INFO - Model: \"LSTM_42113_2023-09-20--15-39\"\n",
      "2023-09-20 15:39:27 - __main__ - INFO - _________________________________________________________________\n",
      "2023-09-20 15:39:27 - __main__ - INFO -  Layer (type)                Output Shape              Param #   \n",
      "2023-09-20 15:39:27 - __main__ - INFO - =================================================================\n",
      "2023-09-20 15:39:27 - __main__ - INFO -  lstm (LSTM)                 (None, None, 64)          17152     \n",
      "2023-09-20 15:39:27 - __main__ - INFO -                                                                  \n",
      "2023-09-20 15:39:27 - __main__ - INFO -  lstm_1 (LSTM)               (None, 32)                12416     \n",
      "2023-09-20 15:39:27 - __main__ - INFO -                                                                  \n",
      "2023-09-20 15:39:27 - __main__ - INFO -  dense (Dense)               (None, 128)               4224      \n",
      "2023-09-20 15:39:27 - __main__ - INFO -                                                                  \n",
      "2023-09-20 15:39:27 - __main__ - INFO -  dense_1 (Dense)             (None, 64)                8256      \n",
      "2023-09-20 15:39:27 - __main__ - INFO -                                                                  \n",
      "2023-09-20 15:39:27 - __main__ - INFO -  dense_2 (Dense)             (None, 1)                 65        \n",
      "2023-09-20 15:39:27 - __main__ - INFO -                                                                  \n",
      "2023-09-20 15:39:27 - __main__ - INFO - =================================================================\n",
      "2023-09-20 15:39:27 - __main__ - INFO - Total params: 42113 (164.50 KB)\n",
      "2023-09-20 15:39:27 - __main__ - INFO - Trainable params: 42113 (164.50 KB)\n",
      "2023-09-20 15:39:27 - __main__ - INFO - Non-trainable params: 0 (0.00 Byte)\n",
      "2023-09-20 15:39:27 - __main__ - INFO - _________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------Model Architecture--------------------------\n",
    "model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(None,2)),\n",
    "        tf.keras.layers.LSTM(32),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1),\n",
    "        ],\n",
    "    name=config['model']['name'])\n",
    "\n",
    "model._name = f\"{model._name}_{str(model.count_params())}_{datetime.now().strftime('%Y-%m-%d--%H-%M')}\"\n",
    "log_model_info(config, model, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "178/178 [==============================] - 7s 16ms/step - loss: 40.1308 - mae: 40.6212\n",
      "Epoch 2/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 37.2552 - mae: 37.7416\n",
      "Epoch 3/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 26.4977 - mae: 26.9805\n",
      "Epoch 4/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 3.5514 - mae: 4.0071\n",
      "Epoch 5/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.9579 - mae: 2.3888\n",
      "Epoch 6/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.9249 - mae: 2.3503\n",
      "Epoch 7/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.4547 - mae: 1.8588\n",
      "Epoch 8/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.4102 - mae: 1.8086\n",
      "Epoch 9/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.2273 - mae: 1.6203\n",
      "Epoch 10/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.3290 - mae: 1.7201\n",
      "Epoch 11/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.2736 - mae: 1.6841\n",
      "Epoch 12/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.0209 - mae: 1.4008\n",
      "Epoch 13/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.0220 - mae: 1.3937\n",
      "Epoch 14/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.2594 - mae: 1.6488\n",
      "Epoch 15/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.0760 - mae: 1.4511\n",
      "Epoch 16/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.9937 - mae: 1.3767\n",
      "Epoch 17/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.0733 - mae: 1.4528\n",
      "Epoch 18/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.0038 - mae: 1.3807\n",
      "Epoch 19/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.8773 - mae: 1.2389\n",
      "Epoch 20/100\n",
      "178/178 [==============================] - 3s 12ms/step - loss: 0.9602 - mae: 1.3292\n",
      "Epoch 21/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.8477 - mae: 1.2067\n",
      "Epoch 22/100\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.0456 - mae: 1.4253\n",
      "Epoch 23/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.7486 - mae: 1.1057\n",
      "Epoch 24/100\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 0.7655 - mae: 1.1165\n",
      "Epoch 25/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.7471 - mae: 1.1074\n",
      "Epoch 26/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.7873 - mae: 1.1484\n",
      "Epoch 27/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.7527 - mae: 1.1148\n",
      "Epoch 28/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.9322 - mae: 1.3085\n",
      "Epoch 29/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.8790 - mae: 1.2464\n",
      "Epoch 30/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.7916 - mae: 1.1538\n",
      "Epoch 31/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.7610 - mae: 1.1171\n",
      "Epoch 32/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.7740 - mae: 1.1332\n",
      "Epoch 33/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.7333 - mae: 1.0977\n",
      "Epoch 34/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.8096 - mae: 1.1681\n",
      "Epoch 35/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.6699 - mae: 1.0240\n",
      "Epoch 36/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.7163 - mae: 1.0754\n",
      "Epoch 37/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.7025 - mae: 1.0540\n",
      "Epoch 38/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.7582 - mae: 1.1241\n",
      "Epoch 39/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.5709 - mae: 0.9009\n",
      "Epoch 40/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.6303 - mae: 0.9693\n",
      "Epoch 41/100\n",
      "178/178 [==============================] - 3s 12ms/step - loss: 0.6315 - mae: 0.9816\n",
      "Epoch 42/100\n",
      "178/178 [==============================] - 3s 13ms/step - loss: 0.6369 - mae: 0.9904\n",
      "Epoch 43/100\n",
      "178/178 [==============================] - 3s 12ms/step - loss: 0.6306 - mae: 0.9626\n",
      "Epoch 44/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.6572 - mae: 1.0008\n",
      "Epoch 45/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.6940 - mae: 1.0352\n",
      "Epoch 46/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.6202 - mae: 0.9651\n",
      "Epoch 47/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.6608 - mae: 1.0043\n",
      "Epoch 48/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.5927 - mae: 0.9260\n",
      "Epoch 49/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.5902 - mae: 0.9212\n",
      "Epoch 50/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.7194 - mae: 1.0803\n",
      "Epoch 51/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.5612 - mae: 0.9017\n",
      "Epoch 52/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.5697 - mae: 0.9037\n",
      "Epoch 53/100\n",
      "178/178 [==============================] - 3s 12ms/step - loss: 0.5249 - mae: 0.8634\n",
      "Epoch 54/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.5364 - mae: 0.8738\n",
      "Epoch 55/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.6975 - mae: 1.0508\n",
      "Epoch 56/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.6684 - mae: 1.0144\n",
      "Epoch 57/100\n",
      "178/178 [==============================] - 3s 12ms/step - loss: 0.5336 - mae: 0.8777\n",
      "Epoch 58/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.5281 - mae: 0.8455\n",
      "Epoch 59/100\n",
      "178/178 [==============================] - 3s 12ms/step - loss: 0.6017 - mae: 0.9557\n",
      "Epoch 60/100\n",
      "178/178 [==============================] - 2s 12ms/step - loss: 0.5408 - mae: 0.8694\n",
      "Epoch 61/100\n",
      "178/178 [==============================] - 3s 12ms/step - loss: 0.5181 - mae: 0.8514\n",
      "Epoch 62/100\n",
      "178/178 [==============================] - 3s 13ms/step - loss: 0.5183 - mae: 0.8456\n",
      "Epoch 63/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.5961 - mae: 0.9370\n",
      "Epoch 64/100\n",
      "178/178 [==============================] - 3s 14ms/step - loss: 0.5180 - mae: 0.8400\n",
      "Epoch 65/100\n",
      "178/178 [==============================] - 3s 11ms/step - loss: 0.4439 - mae: 0.7459\n",
      "Epoch 66/100\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.6445 - mae: 0.9973\n",
      "Epoch 67/100\n",
      " 74/178 [===========>..................] - ETA: 1s - loss: 0.5076 - mae: 0.8426"
     ]
    }
   ],
   "source": [
    "# Set the training parameters and train the model\n",
    "model.compile(loss=config['model']['loss'], \n",
    "            optimizer=config['model']['optimizer'], \n",
    "            metrics=[\"mae\"],\n",
    "            )    \n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=config['model']['epochs'])\n",
    "\n",
    "# Plot MAE and Loss\n",
    "mae=history.history['mae']\n",
    "loss=history.history['loss']\n",
    "zoom = int(len(mae) * 0.9)\n",
    "V.plot_series(x=range(config['model']['epochs'])[-zoom:],\n",
    "                y=(mae[-zoom:],loss[-zoom:]),\n",
    "                model_name=config['model']['name'],\n",
    "                title='MAE_and_Loss',\n",
    "                xlabel='MAE',\n",
    "                ylabel='Loss',\n",
    "                legend=['MAE', 'Loss']\n",
    "            )\n",
    "\n",
    "# Save the model\n",
    "FE.model_save(model, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forecast(model, df, window_size, batch_size):\n",
    "    # calculate actual prediction size \n",
    "    prediction_size = len(series) - window_size\n",
    "    print('prediction_size', prediction_size)\n",
    "\n",
    "    # calculate number of points we need to add to make series evenly divisible by window_size\n",
    "    remainder = prediction_size % window_size\n",
    "    remainder = window_size - remainder if remainder > 0 else 0\n",
    "    print('remainder', remainder)\n",
    "\n",
    "    # get last prediction_size + remainder points from series\n",
    "    series = series[-(prediction_size + remainder):]\n",
    "    print('Paddded series to predict on', series)\n",
    "\n",
    "    # Generate a TF Dataset from the series values\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    print('--------------------------from_tensor_slices--------------------------')\n",
    "    for i,element in enumerate(dataset):\n",
    "        print(element)\n",
    "        print(i)\n",
    "    print('-'*100)\n",
    "\n",
    "    # Batch the data to window size\n",
    "    dataset = dataset.batch(window_size)\n",
    "    print('--------------------------------Batch to window size--------------------------------')\n",
    "    for i,x in enumerate(dataset):\n",
    "        print(i,x)\n",
    "    print('-'*100)\n",
    "\n",
    "\n",
    "    # Create batches of windows\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    print('--------------------------------Batch to Batch size-----------------------------------')\n",
    "    for x in dataset:\n",
    "        print(x)\n",
    "        break\n",
    "    print('-'*100)\n",
    "  \n",
    "    # Get predictions on the entire dataset\n",
    "    forecast = model.predict(dataset)\n",
    "    print('--------------------------------forecast-----------------------------------')\n",
    "    for i,x in enumerate(forecast):\n",
    "        if i > 1:\n",
    "            break\n",
    "        print(i,x)\n",
    "    print('-'*100)\n",
    "\n",
    "    forecast = forecast.squeeze()\n",
    "\n",
    "    # flatten the forecast array\n",
    "    forecast = forecast.flatten()\n",
    "    # get the last prediction_size points\n",
    "    forecast = forecast[-prediction_size:]\n",
    "\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------Predictions---------------------------------\n",
    "forecast_series = df.iloc[-test_size_int - config['model']['window']:]\n",
    "\n",
    "results = model_forecast(model=model, \n",
    "                            series=forecast_series, \n",
    "                            window_size=config['model']['window'], \n",
    "                            batch_size=config['model']['batch_size'])\n",
    "print(results.shape)\n",
    "# unnormalize the data\n",
    "# results = scaler.inverse_transform(results.reshape(-1,1))\n",
    "\n",
    "V.plot_series(  x=df_test.index, \n",
    "                y=(df_test['Adj Close'], results),\n",
    "                model_name=config['model']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------Calculate Errors----------------------------------\n",
    "naive_forecast = get_naive_forecast(df).iloc[-len(df_test['Adj Close']):]\n",
    "rmse, mae, mape, mase = calc_errors(df_test['Adj Close'], results, naive_forecast)\n",
    "save_errors_to_table(config['model']['name'], {'rmse': rmse, 'mae': mae, 'mape': mape, 'mase': mase})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
